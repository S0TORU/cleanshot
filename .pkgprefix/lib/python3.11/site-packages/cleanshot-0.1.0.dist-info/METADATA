Metadata-Version: 2.4
Name: cleanshot
Version: 0.1.0
Summary: Dataset cleaning CLI powered by DuckDB.
Author: Aanu Oshakuade
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: duckdb>=0.9
Dynamic: license-file

# CleanShot

CleanShot is a dataset-cleaning CLI powered by DuckDB. Point it at a CSV, Parquet, or JSON file and it returns a cleaned Parquet file plus a Markdown report. It also has an ML feature pack for outliers, scaling, encoding, splits, and sensor fusion.

## What it does

- Drops mostly-null or constant columns.
- Casts date-like strings to DATE.
- Cleans currency strings into numeric values.
- Imputes missing values (median for numeric, mode for categorical) with `ml_ready`.
- Deduplicates rows.
- Clips outliers by percentile or z-score.
- Adds scaled numeric columns (standardize or min-max).
- Encodes categoricals (one-hot or frequency).
- Splits train/val/test with optional stratification.
- Merges sensor feeds by ID + timestamp tolerance.
- Exports schema and stats as JSON.

## Install

```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip wheel
pip install -e .
```

After publishing to PyPI, install with:

```bash
pip install cleanshot
```

## Quick start (simple)

```bash
cleanshot samples/example_small.csv example_clean.parquet
```

## Quick start (no-questions interactive)

```bash
cleanshot --interactive
```

## Example files

- `samples/example_small.csv` is a trimmed healthcare claims sample (~5k rows).
- `samples/dirty.csv` is a small messy example.

## Easy mode walkthrough

1) Pick your input file.
2) Pick your output file name.
3) Choose your mode: Quick Clean or ML-Ready.
4) (Optional) Turn on ML features like outlier clipping, encoding, scaling, or train/val/test splits.

CleanShot writes a new `.parquet` file and prints a report at the end.

All commands in this README can be run either as `cleanshot ...` (installed CLI) or
`python cleanshot.py ...` (direct script run).

## Hardcore usage

### Inputs

- Single file: `data/my_file.csv`
- Directory: `data/` (same extension files only)
- Glob: `data/**/*.csv`

### Outputs

- Default output is `cleaned.parquet`.
- For a file input like `data/my_file.csv`, the default becomes `my_file_clean.parquet`.

### Presets

- `basic`: drop garbage columns, cast dates, clean currencies, dedup.
- `ml_ready`: all of the above plus imputation.

### Power flags

```bash
# inspect only, no write
python cleanshot.py data.csv --profile

# dry run (build pipeline, no output)
python cleanshot.py data.csv cleaned.parquet --dry-run

# skip imputation
python cleanshot.py data.csv cleaned.parquet --preset basic

# force currency locale
python cleanshot.py data.csv cleaned.parquet --currency-locale us

# reduce inference cost by sampling
python cleanshot.py data.csv cleaned.parquet --sample-rows 5000

# dedup by specific keys
python cleanshot.py data.csv cleaned.parquet --dedup-keys claim_id,member_id

# skip dedup entirely
python cleanshot.py data.csv cleaned.parquet --no-dedup

# clip outliers (percentile or z-score)
python cleanshot.py data.csv cleaned.parquet --outlier-mode percentile --outlier-pct 0.01,0.99
python cleanshot.py data.csv cleaned.parquet --outlier-mode zscore --outlier-zscore 3.0

# add scaled columns
python cleanshot.py data.csv cleaned.parquet --scale standardize
python cleanshot.py data.csv cleaned.parquet --scale minmax

# encode categoricals
python cleanshot.py data.csv cleaned.parquet --encode onehot --encode-max-categories 25
python cleanshot.py data.csv cleaned.parquet --encode frequency

# train/val/test splits
python cleanshot.py data.csv cleaned.parquet --split 0.8,0.1,0.1
python cleanshot.py data.csv cleaned.parquet --split 0.8,0.1,0.1 --split-stratify label

# export schema
python cleanshot.py data.csv cleaned.parquet --schema-out schema.json

# write the Markdown report to a file
python cleanshot.py data.csv cleaned.parquet --report-out report.md

# threads + compression
python cleanshot.py data.csv cleaned.parquet --threads 4 --compression zstd
```

### Hardcore examples

```bash
# run the full pipeline with ML imputation
python cleanshot.py samples/example_small.csv example_clean.parquet --preset ml_ready --threads 4

# inspect first, then pick a preset
python cleanshot.py samples/example_small.csv --profile

# add encoding + scaling for ML
python cleanshot.py samples/example_small.csv example_clean.parquet --encode onehot --scale standardize --target label

# split train/val/test and export schema
python cleanshot.py samples/example_small.csv example_clean.parquet --split 0.8,0.1,0.1 --schema-out schema.json

# sample a giant dataset
python cleanshot.py data/big/*.csv cleaned.parquet --sample-rows 100000

# use a directory of CSVs
python cleanshot.py data/ cleaned.parquet
```

## Output report

A report is printed at the end of every run and includes:

- Rows in/out and columns in/out
- Columns dropped, date-cast, currency-cleaned
- Which imputation happened (if any)
- Outlier, scaling, encoding steps
- Split details (if enabled)
- Dedup results

To save the report to a file:

```bash
python cleanshot.py data.csv cleaned.parquet --report-out report.md
```

## Sensor fusion

Use this when you have multiple sensor files with a shared ID and timestamp column.

```bash
python cleanshot.py --fusion-source imu=sensors/imu.csv --fusion-source gps=sensors/gps.csv \
  --fusion-id-col device_id --fusion-time-col timestamp --fusion-time-tolerance 500ms \
  --output fused_clean.parquet
```

## Troubleshooting

- `input is required unless --interactive` means you forgot the input file.
- `Mixed extensions` means the folder/glob has different file types.
- `Unsupported extension` means you used a file type that is not CSV, Parquet, or JSON.
- `Fusion requires --fusion-id-col and --fusion-time-col` means you forgot fusion join keys.

## Contributing

See `CONTRIBUTING.md` for development setup, style, and PR guidelines.

## Author

Aanu Oshakuade

## License

MIT
